{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "### 1. What is Embedding Layer\n",
    "### 2. Embedding Implementation - Scratch\n",
    "### 3. Embedding Implementation - PyTorch - ``torch.nn.Embedding()``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is embedding layer?\n",
    "\n",
    "### Purpose & Role\n",
    "입력되는 값(보통은 one-hot encoded된 sparse한 정수 vector/matrix)을 dense vector로 mapping하고, 이 dense vector가 다시 neural net의 input으로 들어간다.\n",
    "\n",
    "이를 통해 sparse하지 않게, 특정 성질(word embedding에서는 각 단어, NCF에서는 user나 item)을 상대적으로 low dimensiond의 dense vector로 바꿔 줄 수 있다는 장점이 있다.\n",
    "\n",
    "\n",
    "### Usage\n",
    "* Word Embedding\n",
    "    - 각 단어들을 n차원의 dense vector로 mapping해준다.\n",
    "* RecSys - Neural Collaborative Filtering\n",
    "    - user와 item은 각각 one-hot encoding된 vector로 표현된다. 이들은 embedding 층을 거쳐 각각이 dense한 vector로 표현된다. 이 때,  dense vector의 각 value는 latent factor의 value로 해석 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Implementation - Scratch\n",
    "**Word Embedding Case**\n",
    "\n",
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 2, 'how': 3, 'need': 4, 'to': 5, 'you': 6, 'know': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data= 'you need to know how to code'\n",
    "word_set= set(train_data.split())\n",
    "vocab= {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>']= 0\n",
    "vocab['<pad>']= 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "embedding_table= torch.FloatTensor([\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [0.2, 0.9, 0.3],\n",
    "    [0.1, 0.5, 0.7],\n",
    "    [0.2, 0.1, 0.8],\n",
    "    [0.4, 0.1, 0.1],\n",
    "    [0.1, 0.8, 0.9],\n",
    "    [0.6, 0.1, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word/sentence to embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.8000, 0.9000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.4000, 0.1000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "sample= 'you need to run'.split()\n",
    "idxes= []\n",
    "\n",
    "for word in sample:\n",
    "    try:\n",
    "        idxes.append(vocab[word])\n",
    "    except KeyError:\n",
    "        idxes.append(vocab['<unk>'])\n",
    "        \n",
    "idxes= torch.LongTensor(idxes)\n",
    "\n",
    "look_up= embedding_table[idxes, :]\n",
    "print(look_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Embedding Implementation - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 2, 'how': 3, 'need': 4, 'to': 5, 'you': 6, 'know': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data= 'you need to know how to code'\n",
    "word_set= set(train_data.split())\n",
    "vocab= {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>']= 0\n",
    "vocab['<pad>']= 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``nn.Embedding()``\n",
    "* ``num_embeddings``: embedding 할 item의 개수 - embedding layer의 row 수 처럼 생각\n",
    "* ``embedding_dim``: embedding 할 vector의 차원 - embedding layer의 column 수 처럼 생각\n",
    "* ``padding_idx``: (optional) 해당 index는 embedding 값을 0으로 설정(아래 예시 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "embedding_layer= nn.Embedding(num_embeddings= len(vocab),\n",
    "                             embedding_dim= 3,\n",
    "                             padding_idx= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0610,  0.6827, -1.1725],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.7623,  0.1139,  1.1588],\n",
      "        [ 0.6956,  0.4195, -0.0237],\n",
      "        [-1.0562, -1.9752,  0.6914],\n",
      "        [ 0.1812,  0.6025,  0.1532],\n",
      "        [ 0.3423, -0.1915, -2.1392],\n",
      "        [ 0.6244,  0.2271, -0.5618]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
