{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net을 scratch로 구현해보고, 구현한 Neural Net의 부분 부분을 ``torch.nn``의 class와 method로 대체해가면서 어떠한 class/method가 scratch code의 어떠한 부분을 대체하는 지를 익힌다.\n",
    "\n",
    "여러모로 **'02_Neural_Networks'**와 비슷한 내용을 담고있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "### Part 0: Neural Net from Scratch\n",
    "* MNIST  Data Setup\n",
    "* Neural Net from Scratch (w/o torch.nn)\n",
    "\n",
    "### Part 1: Simplifying Training Session with ``Torch.nn``\n",
    "* **``torch.nn.functional``**\n",
    "* **``nn.Module``**\n",
    "* **``nn.Linear``**\n",
    "* **``torch.optim``**\n",
    "* **``torch.utils.data.TensorData``**\n",
    "* **``torch.utils.data.DataLoader``**\n",
    "\n",
    "### Part 2: Validation\n",
    "#### Adding Validation\n",
    "* Shuffling\n",
    "* batch size\n",
    "* ``model.train()`` & ``model.eval()``\n",
    "\n",
    "### Part 3: Applying to CNN\n",
    "* **Modeling CNN**\n",
    "* **``nn.Sequential``**\n",
    "\n",
    "\n",
    "### Summary (Important!)\n",
    "#### How to Design NN\n",
    "* Algortihms\n",
    "* Using **``torch.nn``**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Neural Net from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "\n",
    "DATA_PATH= Path(\"data\")\n",
    "PATH= DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents= True, exist_ok= True)\n",
    "\n",
    "URL= \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME= \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content= requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)\n",
    "    \n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "        \n",
    "x_train, y_train, x_valid, y_valid= map(\n",
    "    torch.tensor,  (x_train,y_train, x_valid, y_valid)\n",
    ")\n",
    "\n",
    "n, c= x_train.shape\n",
    "x_train,  x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(),  y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural Net from Scratch (w/o torch.nn)\n",
    "create Neural Net using only PyTorch Tensor operations(without ``torch.nn`` pacakge).\n",
    "\n",
    "### Before Training\n",
    "* weights, bias 초기화\n",
    "* activation function\n",
    "* model(linear layer)\n",
    "    * linear layer에서 진행되는 computation을 정의해야 함.\n",
    "    * m\n",
    "* loss function\n",
    "* accuracy function\n",
    "\n",
    "### Training\n",
    "매 epoch마다\n",
    "* 학습 시킬 batch (x & y) \n",
    "* x로 prediction 진행\n",
    "* x, y로 loss 계산\n",
    "* ``.backward()``\n",
    "* weight, bias update (with ``torch.no_grad`` b/c we shouldn't consider this process when calculating gradient)\n",
    "* weight, bias grad_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights & bias\n",
    "import math\n",
    "\n",
    "weights= torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias= torch.zeros(10, requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4852, -2.1391, -2.6954, -2.4873, -2.1909, -2.0923, -2.2463, -2.4395,\n",
      "        -2.2207, -2.1934], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# model(linear layer)\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)\n",
    "\n",
    "bs= 64    # batch size\n",
    "xb= x_train[:bs]\n",
    "preds= model(xb)\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4059, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# loss function - negative log likelihood \n",
    "# 즉, negative log likelihood를 최소화 하는 문제\n",
    "def nll(input_, target):\n",
    "    return -input_[range(target.shape[0]), target].mean()\n",
    "           # tensor_a[(0, 1), (3, 2)] returns tensor_a[0][3] and tensor_a[1][2]\n",
    "\n",
    "loss_func= nll\n",
    "\n",
    "yb= y_train[:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Question</h4><p> nll이 왜 저렇게 생겼지? </p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0781)\n"
     ]
    }
   ],
   "source": [
    "# accruacy function\n",
    "def accuracy(out, yb):\n",
    "    preds= torch.argmax(out, dim=1)\n",
    "    return (preds==yb).float().mean()\n",
    "\n",
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.5\n",
    "epochs= 2\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i= i*bs\n",
    "        end_i= start_i + bs\n",
    "        xb= x_train[start_i:end_i]\n",
    "        yb= y_train[start_i:end_i]\n",
    "        pred= model(xb)\n",
    "        loss= loss_func(pred, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weights -= weights.grad * lr\n",
    "        bias -= bias.grad * lr\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.8000306487083435 \n",
      "accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"loss: \", loss_func(model(xb), yb).item(), \"\\naccuracy: \",accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Simplifying Training Session\n",
    "## ``torch.nn.functional``\n",
    "``torch.nn.functional``은 ``torch.nn``의 모든 function들을 담고 있는 패키지이다(나머지는 모두 class를 담고있음).\n",
    "\n",
    "functional의 function은\n",
    "* loss function\n",
    "* activation function\n",
    "* other functions(pooling function, relu, ...)\n",
    "등이 있다.\n",
    "\n",
    "### Replace\n",
    "negative log likelihood(loss function)과 log sogtmax(activation function) 대신, ``F.cross_entropy``를 사용 할 수 있다.\n",
    "\n",
    "즉, **loss function**과 **activation function**을 **``torch.nn.functional``**의 method로 대체할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original loss f, activation f, and model\n",
    "'''\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)\n",
    "    \n",
    "def nll(input_, target):\n",
    "    return -input_[range(target.shape[0]), target].mean()\n",
    "'''\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func= F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.800030529499054 \n",
      "accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"loss: \", loss_func(model(xb), yb).item(), \"\\naccuracy: \",accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``nn.Module``\n",
    "``nn.Module``을 통해 model Class를 정의하면 **weight/bias initalizing**과 **Forward Pass**를 쉽게 구현 할 수 있다.\n",
    "\n",
    "* ``nn.Parameter``: module의 attribute로 ``nn.Paramter``를 assign한다면, 자동으로 모델의 **parameter**로 인식되며, ``.parameters()`` iterator를 통해 확인 할 수 있다. 이를 통해, weights와 bias를 따로따로 지정하지 않고 모델의 parameter들을 한번에 update할 수 있다. 특히, 여러개의 layer가 겹겹히 쌓인 model의 경우 훨씬 더 유용하다.\n",
    "* ``model.zero_grad()``를 통해 각 parameter에 쌓인 gradient를 초기화 할 수 있다(매 iteration마다 gradient 초기화 필요)\n",
    "\n",
    "### Replace\n",
    "weight & bias 초기화, training 과정에서의 forward pass 대신, **``nn.Module``을 상속하는 Class를 정의할 수 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# class - initiailzing weights & bias + defining forward pass\n",
    "class Mnist_Logistics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights= nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias= nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Mnist_Logistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            start_i= i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb= x_train[start_i:end_i]\n",
    "            yb= y_train[start_i:end_i]\n",
    "            \n",
    "            pred= model(xb)\n",
    "            # we previously defined loss_func as F.cross_entropy\n",
    "            loss= loss_func(pred, yb)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                # don't have to update weights and bias separately\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "            \n",
    "            model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0784, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``nn.Linear``\n",
    "[``nn.Linear``](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)를 통해 weights와 bias를 initalize하고 forward pass 식을 간소화 할 수 있다.\n",
    "\n",
    "당연한 얘기지만, linear layer기 때문에 ``nn.Linear``를 사용하는 것. 필요한 layer의 성격에 따라 다양한 method를 사용하거나, ``nn.Conv2d``와 같은 class를 사용 할 수 있다.\n",
    "\n",
    "### Replace\n",
    "다음의 과정들을 nn.Linear를 통해 구현 할 것이다.\n",
    "* class의 ``__init__()``에서 weigths와 bias를 초기화 하는 과정 - bias도 자동으로 만들어준다.\n",
    "* class의 ``forward()``의 forward pass 계산 식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       #self.weights= nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "       #self.bias= nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "        self.lin= nn.Linear(784, 10)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "      # return xb @ self.weights + self.bias\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0811, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model= Mnist_Logistics()\n",
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ``torch.optim``\n",
    "[``torch.optim``](https://pytorch.org/docs/stable/optim.html)을 통해 backward pass와 weights update과정을 간소화 할 수 있다.\n",
    "\n",
    "``torch.optim``에는 다양한 optimizer가 담겨있다. \n",
    "\n",
    "optimizer를 생성 할 때에는 optimizer를 적용하고자 하는 model의 parameter를 ``model.parameters()``를 통해 전달해 준다(with learning rate).\n",
    "\n",
    "각각의 optimizer는 다음과 같은 method를 수행 할 수 있다.\n",
    "* ``.step()``:  ``.backward()``를 실행한 후, 그 결과를 바탕으로 **weights update**를 진행한다.\n",
    "* ``.zero_grad()``: weights & bias에 쌓인 gradient를 초기화한다.\n",
    "\n",
    "### Replace\n",
    "다음의 과정들을 ``torch.optim``을 통해 구현 할 것이다.\n",
    "* optimizer 생성\n",
    "* fit()에서, backward pass이후 weight을 update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# model과 함께 optimizer를 생성\n",
    "def get_model():\n",
    "    model= Mnist_Logistics()\n",
    "    return model, optim.SGD(model.parameters(), lr= lr)\n",
    "\n",
    "model, opt= get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i= i * bs\n",
    "        end_i= start_i + bs\n",
    "        xb= x_train[start_i:end_i]\n",
    "        yb= y_train[start_i:end_i]\n",
    "        pred= model(xb)\n",
    "        loss= loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        # .backward()가 진행되었으므로, 이를 통해 weight update\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0821, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ``torch.utils.data.TensorDataset``\n",
    "\n",
    "Pytorch는 ``DataSet`` Class가 있다. ``DataSet`` class는 1) ``__len__`` function과 2) ``__getitem__`` function (as a way of indexing into it)이 있는 모든 class를 일컫는다.\n",
    "\n",
    "특히 이 중 [``torch.utils.data.TensorDataset``](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)의 경우, **Tensor**들을 **wrapping**하는 DataSet이다. \n",
    " \n",
    "### Replace\n",
    "\n",
    "TensorDataset을 indexing하면, 각 tensor들이 first dimension을 기준으로 indexing되어 반환된다.\n",
    "이는 NN 모델에서는 당연히, **x와 y를 같은 batch size로 indexing 할 때**에 사용된다.\n",
    "1. 기존 indexing 방법\n",
    "\n",
    " xb= x_train[start_i: end_i]\n",
    ", yb= y_train[start_i: end_i]\n",
    "\n",
    "\n",
    "2. TensorDataset을 이용한 indexing 방법\n",
    "\n",
    " train_ds= TensorDataset(x_train, y_train)\n",
    "\n",
    " xb, yb= train_ds[start_i: end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0812, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# TensorDataset 생성\n",
    "train_ds= TensorDataset(x_train, y_train)\n",
    "\n",
    "model, opt= get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        '''start_i= i * bs\n",
    "        end_i= start_i + bs\n",
    "        xb= x_train[start_i:end_i]\n",
    "        yb= y_train[start_i:end_i]'''\n",
    "        # TensorDataset의 특성을 이용해 x와 y를 한번에 return 받는다.\n",
    "        xb, yb= train_ds[i*bs:i*bs+bs]\n",
    "        pred= model(xb)\n",
    "        loss= loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ``torch.utils.data.DataLoader``\n",
    "\n",
    "[``torch.utils.data.DataLoader``](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)는 batching을 하기 위해 특화된 PyTorch의 class. DataSet과 batch_size(1 if None)가 주어지면 그 size에 맞게 DataSet의 element들을 indexing하여 반환하는 iterator이다.\n",
    "\n",
    "shuffle=True로 설정 할 시, dataset을 반환할 때 shuffle을 한 후 반환한다.\n",
    "### Replace\n",
    "batching 과정이 더더욱 간소화된다.\n",
    "1. 기존 batching 방법\n",
    "\n",
    "    for i in range((n-1) // bs +1)):\n",
    "        xb, yb= train_ds[i*bs: i*bs+bs]\n",
    "        pred= model(xb)\n",
    "\n",
    "2. DataLoader를 이용한 batching 방법\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "        pred= model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0828, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds= TensorDataset(x_train, y_train)\n",
    "# create DataLoader with TensorDataset and it's batch size\n",
    "train_dl= DataLoader(train_ds, batch_size= bs)\n",
    "\n",
    "model, opt= get_model()\n",
    "for epoch in range(epochs):\n",
    "    # DataLoader is iteratior\n",
    "    for xb, yb in train_dl:\n",
    "        pred= model(xb)\n",
    "        loss= loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Until here, using ``nn.functional``, ``nn.Module``, ``nn.Linear``, ``torch.optim``, ``TensorDataset``, ``DataLoader``, we simplified training session dramatically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Validation\n",
    "## Add Validation\n",
    "\n",
    "**Validation**은 Overfitting을 방지하기 위해 반드시 필요하다.\n",
    "\n",
    "Neural Net에 validation 과정을 도입 할 때 고려 할 사항이 몇가지 있다.\n",
    "\n",
    "### Shuffling\n",
    "training data의 경우, overfitting을 방지하기 위해 batch를 생성 할 때 shuffle을 해주어야 한다.\n",
    "그러나 validation data의 경우, **학습**하는 것이 아니므로, shuffle을 할 필요가 없다. 쓸데없는 연산을 줄이기 위해, validation batch를 생성 할 때 shuffle을 꺼주어야 한다.\n",
    "\n",
    "### batch size\n",
    "validation set의 경우 back propagation을 진행 할 필요가 없다(forward pass만을 진행하여 결과값만 구하면 된다). 따라서, 연산량이 훨씬 가볍기 때문에 batch size를 training 과정에서보다 늘려도 된다.\n",
    "\n",
    "### ``model.train()`` & ``model.eval()``\n",
    "``nn.BatchNorm2d``나 ``nn.Dropout``과 같은 layer의 경우, training 과정과 validation 과정에서의 작동 방식이 다르기 때문에 현재 진행하고자 하는 step을 설명해주어야 한다.\n",
    "\n",
    "따라서, training 전에는  ``model.train()``을, validation 전에는 ``model.eval()``을 선언해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling & Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Question</h4><p> Why do we have to batch the validation set? Can't we just compute it altogether? </p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds= TensorDataset(x_train, y_train)\n",
    "train_dl= DataLoader(train_ds, batch_size= bs)\n",
    "\n",
    "valid_ds= TensorDataset(x_valid, y_valid)\n",
    "valid_dl= DataLoader(valid_ds, batch_size= bs*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model.train() & model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.3099\n",
      "epoch: 2, loss: 0.2902\n",
      "epoch: 3, loss: 0.2833\n",
      "epoch: 4, loss: 0.2800\n",
      "epoch: 5, loss: 0.2782\n"
     ]
    }
   ],
   "source": [
    "model, opt= get_model()\n",
    "epochs= 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred= model(xb)\n",
    "        loss= loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss= sum(loss_func(model(xb), yb) for (xb, yb) in valid_dl)\n",
    "        \n",
    "    print(\"epoch: %d, loss: %.4f\" % (epoch+1, (valid_loss.item() / len(valid_dl))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### loss의 함수화\n",
    "loss를 계산하는 과정은 training에서도 validation에서도 사용되므로, 하나의 함수를 정의해 코드의 길이를 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss= loss_func(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size= bs, shuffle= True),\n",
    "        DataLoader(valid_ds, batch_size= bs*2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses, nums= zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for (xb, yb) in valid_dl]\n",
    "            )\n",
    "        val_loss= np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        \n",
    "        print(\"epoch: %d, validaiton loss: %.4f\" % (epoch+1, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, validaiton loss: 0.4107\n",
      "epoch: 2, validaiton loss: 0.2994\n",
      "epoch: 3, validaiton loss: 0.3944\n",
      "epoch: 4, validaiton loss: 0.2811\n",
      "epoch: 5, validaiton loss: 0.2838\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl= get_data(train_ds, valid_ds, bs)\n",
    "model, opt= get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Applying to CNN\n",
    "\n",
    "## Modeling CNN\n",
    "위에서 정의한 함수들(``fit()``과 그 안에 투입되는 함수들)의 경우, 특정 Neural Network 모델을 염두해 두고 만든 것이 아닌, 보편적으로 적용 될 수 있는 함수이다.\n",
    "\n",
    "따라서, 적절한 model을 fit 함수에 집어넣을 경우 다양한 Neural Network을 구현 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, validaiton loss: 0.3867\n",
      "epoch: 2, validaiton loss: 0.2670\n",
      "epoch: 3, validaiton loss: 0.2067\n",
      "epoch: 4, validaiton loss: 0.1875\n",
      "epoch: 5, validaiton loss: 0.1818\n"
     ]
    }
   ],
   "source": [
    "model= Mnist_CNN()\n",
    "opt= optim.SGD(model.parameters(), lr= lr, momentum= 0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ``nn.Sequential``\n",
    "``nn.Sequential``은 안에 담긴 module들을 순차적으로 실행하는 class이다. 일종의 **pipeline**을 만드는 셈\n",
    "\n",
    "model에 function을 사용하고 싶어도 **layer**의 형태를 띄어야 하므로, 아래와 같은 일종의 **가짜 layer를 만들고, 그 안에 넣을 함수도 만든다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func= func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, validaiton loss: 0.3268\n",
      "epoch: 2, validaiton loss: 0.2670\n",
      "epoch: 3, validaiton loss: 0.2085\n",
      "epoch: 4, validaiton loss: 0.2047\n",
      "epoch: 5, validaiton loss: 0.1485\n"
     ]
    }
   ],
   "source": [
    "model= nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size= 3, stride= 2, padding= 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size= 3, stride= 2, padding= 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size= 3, stride= 2, padding= 1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt= optim.SGD(model.parameters(), lr= lr, momentum= 0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Summary (Important!)\n",
    "\n",
    "## How To Design NN\n",
    "I am Supposing we are building NN with single linear layer.\n",
    "\n",
    "### Before Training\n",
    "* Initialize **weights** and **bias**\n",
    "    * by **subclass**ing *``nn.Module``* and  *``nn.Linear``*\n",
    "* Define **Activation Function**\n",
    "    * by *``torch.nn.functional``*\n",
    "* Design **Model** - computation inside layer\n",
    "    * by **subclass**ing *``nn.Module``* and *``nn.Linear``*\n",
    "* Define **Loss Function**\n",
    "    * by *``torch.nn.functional``*\n",
    "\n",
    "### During Training\n",
    "#### In every  epoch,\n",
    "* Make **batch** for training(x_batch, y_batch)\n",
    "    * by *``torch.utils.data.TensorDataset``* and *``torch.utils.data.DataLoader``*\n",
    "* Make **prediction** with x\n",
    "    * by **subclass**ing *``nn.Module``*\n",
    "* Compute **loss**\n",
    "    * by *``torch.nn.functional``*\n",
    "* **backward pass**\n",
    "* **Update** weights and bias -  with ``torch.no_grad``\n",
    "    * by **subclass**ing *``nn.Module``* and *``torch.optim``*\n",
    "* **Zero** gradients of weights and bias\n",
    "    * by **subclass**ing *``nn.Module``* and *``torch.optim``*\n",
    "\n",
    "#### Validation\n",
    "* Make **batch** for validation\n",
    "    * by *``torch.utils.data.TensorDataset``* and *``torch.utils.data.DataLoader``*\n",
    "* Compute **loss**\n",
    "    * by *``torch.nn.functional``*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
