{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### 0. torch.autograd란?\n",
    "### 1. Mathematical & Operational Basis\n",
    "### 2. Autograd 예시\n",
    "### 3. Frozen parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. torch.autograd란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.autograd: Pytorch's automatic differentiation engine**\n",
    "\n",
    "즉, ``autograd``의 역할은 **미분을 하는 것**\n",
    "~~~\n",
    "그렇다면, 왜 미분을 해야하는 걸까?   ->  Back Propagation을 위해서\n",
    "\n",
    "NN을 training 하는 과정 중 하나인 Back Propagation에서, gradient가 필요하다.\n",
    "따라서, autograd를 이용하여 각 parameter에서의 gradient를 계산한다.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical & Operational Basis\n",
    "### 1.1 Mathematical Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조금 더 수학적으로 들어가보면, autograd란 결국 vector-Jacobian product를 계산하는 과정이다. 주어진 gradient($\\vec{v}$)를 받아, \n",
    "$J^{T}\\cdot \\vec{v}$를 계산해 주는 것.\n",
    "\n",
    "link: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Question</h4><p> v의 값을 어떻게 정해야하지? </p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Operational Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor는 연산(by Function)을 통해 다른 tensor를 생성한다.  이 때, input으로 사용된 tensor가 ``requires_grad`` attribute를 True로 갖는다면, 이 tensor로부터 실행되는 연산들이 추적된다. autograd는 추적한 연산들을 DAG(directed acyclic graph)에 저장한다.\n",
    "\n",
    "구체적으로, autgorad는 다음과 같은 과정을 수행한다.\n",
    "1. **Forward Pass**\n",
    "    * 요청된 연산 처리(새로운 tensor 생성)\n",
    "    * ``.grad_fn``에 수행된 연산의 gradient function 기록\n",
    "    \n",
    "2. **Backward Pass**\n",
    "    * ``.grad_fn``을 참고하여 gradient 계산\n",
    "    * 계산한 gradient를 각 tensor의 ``.grad``에 기록\n",
    "    * 위의 두 과정을 chain rule을 이용하여 DAG의 leaf tensor까지 propagate (DAG는 output tensor가 root가 된다)\n",
    "    \n",
    "위 과정을 정리한 이미지: \n",
    "https://pytorch.org/tutorials/_images/dag_autograd.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. autograd 예시\n",
    "\n",
    "### 2.1 autograd differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n",
    "위의 연산을 구현해 보며, autograd의 작동 방식을 이해해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input tensor인 a, b를 초기화. 이 때, ``requires_grad``는 True여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a= torch.tensor([2., 3.], requires_grad=True)\n",
    "b= torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Q를 a와 b의 연산으로 정의. Q는 자연스레 ``requires_grad``의 값을 True로 가지며, autograd에 의해 어떠한 연산이 이루어졌는지 ``.grad_fn``에 기록된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q= 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad of Q:  True\n",
      ".grad_fn of Q:  <SubBackward0 object at 0x7ffa8208cdc0>\n"
     ]
    }
   ],
   "source": [
    "print(\"requires_grad of Q: \",Q.requires_grad)\n",
    "print(\".grad_fn of Q: \",Q.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Q에 backward()를 call하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad= torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 training 과정을 보면서 autograd가 NN의 training 과정에서 어떤 역할을 하는 지 이해해보자\n",
    "\n",
    "0. 예시에 사용할 pre-trained model을 부르고 데이터를 random으로 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model= torchvision.models.resnet18(pretrained=True)\n",
    "data= torch.rand(1, 3, 64, 64)\n",
    "labels= torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forward Pass (predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Forward pass의 결과값으로부터 loss function을 구하고, backward pass 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x7ffa8208cbe0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. optimizer의 .step()을 통해 계산된 gradient를 이용하여 parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim= torch.optim.SGD(model.parameters(), lr=1e-2, momentum= 0.9)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Frozen Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor의 ``requires_grad``가 False인 경우, 연산들이 추적되지 않는다(= gradient가 계산되지 않는다). Neural Network에서, gradient를 추적하지 않는 parameter를 **Frozen Parameter**라고 부른다. 몇몇 경우, 의도적으로 parameter를 freeze하는 경우가 있다.\n",
    "\n",
    "1. Computational Benefit\n",
    "    계산량을 줄임으로서 조금 더 빠르게 연산을 처리 할 수 있다.\n",
    "2. Finetuning\n",
    "    Finetuning의 과정에서는, 특정 layer만 train하는 경우가 있다. 이 때, 나머지 parameter들을 freeze!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
