{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구조적 API\n",
    "구조적 API란?: 데이터의 흐름을 정의하는 기본 추상화 개념. 비정형 로그 파일부터 반정형 csv 파일, 정형적인 Parquet 파일까지 다양한 유형의 데이터를 처리 할 수 있는 도구.\n",
    "\n",
    "#### 구조적 API의 종류\n",
    "* Dataset\n",
    "* DataFrame\n",
    "* SQL Table & View\n",
    "\n",
    "#### Schema\n",
    "Schema: DataFrame의 column name과 data type을 정의. \n",
    "1. 데이터소스에서 얻거나\n",
    "2. 직접 정의 할 수 있음\n",
    "\n",
    "### Dataset vs DataFrame\n",
    "* Dataset: '타입형' - 스키마의 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인\n",
    "    * JVM기반의 언어인 스칼라와 자바에서만 지원\n",
    "* DataFrame: '비타입형' - 스키마에 명시된 데이터 타입의 일치 여부를 런타임에 확인\n",
    "    * Python을 포함한 대부분의 언어에서 지원\n",
    "    \n",
    "스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 매핑된다.\n",
    "\n",
    "따라서, DataFrame을 다른 언어로 작성하여 실행하여도 해당 데이터 타입이 스파크의 데이터 타입과 매핑되어, 스파크의 연산이 수행된다.\n",
    "\n",
    "아래의 코드 예제도 파이썬이 아닌 **스파크**의 덧셈 연산으로 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"] + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 구조적 API의 실행 과정\n",
    "구체적으로 스파크 코드가 클러스터에서 어떤 과정을 거처 실행되는 지 알아보자. 크게는 다음과 같다.\n",
    "0. DataFrame/Dataset/SQL을 이용한 코드 작성\n",
    "1. (정상적인 코드라면) 스파크가 코드를 **논리적 실행 계획**으로 변환\n",
    "2. 스파크는 **논리적 실행 계획**을 **물리적 실행 계획**으로 변환하며, 추가적인 최적화 가능성 탐색\n",
    "3. 클러스터에서 물리적 실행 계획(RDD 처리)를 실행\n",
    "\n",
    "### 1. 논리적 실행 계획\n",
    "사용자의 코드를 최적화된 **논리적 실행 계획**으로 바꾸는 단계.\n",
    "\n",
    "1. 사용자의 코드를 추상적 트랜스포메이션으로 변환. 코드의 유효성/테이블, 컬럼의 존재 여부만을 판단 - **검증 전 논리적 실행 계획**\n",
    "2. 스파크 분석기: 필요한 테이블과 컬럼을 구체적으로 검증 - **검증된 논리적 실행 계획**\n",
    "3. 카탈리스트 optimizer: 논리적 실행 계획을 최적화 - **최적화된 논리적 실행 계획**\n",
    "\n",
    "### 2. 물리적 실행 계획\n",
    "논리적 실행 계획을 클러스터 환경에서 실행하는 방법 정의.\n",
    "\n",
    "다양한 물리적 실행 전략을 생성하고 비용 모델을 이용하여 비교한 후 최적의 전략 선택.\n",
    "\n",
    "이러한 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "MAX_MEMORY = \"32g\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Test Shell\")\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
