{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "### 1. Matrix Factorization\n",
    "* 기본 아이디어\n",
    "* user/item bias\n",
    "* Algorithm\n",
    "\n",
    "### 2. Training_Test Simultaneously\n",
    "\n",
    "### 3. Parameter Tuning\n",
    "* optimal K\n",
    "* optimal # of Iterations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "### 기본 아이디어\n",
    "user와 item의 관계에 있어서, 둘을 이어주는 잠재 요인(**latent factor**)가 있을 것이다. 즉,\n",
    "\n",
    "* item: 각각의 item들은 여러 특성을 가지고 있고, 그 특성은 몇개의 **latent factor**로 표현될 수 있다.\n",
    "* user: 각각의 user들은 그들의 취향을 가지고 있고, 그 취향은 몇개의 **latent factor**로 설명될 수 있다.\n",
    "\n",
    "따라서, **Matrix Factorization**은 그 둘을 잇는 latent factor와 P(user - latent factor relation), Q(item - latent factor의 relation)을 찾는 과정이다(물론 여느 알고리즘처럼 각각의 latent factor가 무엇을 의미하는 지는 알 수 없다).\n",
    "\n",
    "### user/item bias\n",
    "추가로, 마치 Collaborative Filtering에서 user-bias를 고려하였듯이, 예상 평점을 추측할 때 user와 item의 경향성을 함께 고려한다. 따라서, 예상 평점을 구하는 식은 다음과 같다.\n",
    "\n",
    "``예상 평점 = 전체 평균(b) + user 경향성(b_u) + item 경향성(b_d) + **취향**(P & Q)``\n",
    "\n",
    "### Algorithm\n",
    "1. K(# of latent factors)를 정한다. (후에 parameter tuning을 통해 최적 값을 찾는 과정을 거친다)\n",
    "2. 주어진 K에 따라 행렬 P(m x k), Q(n x k)를 초기화한다.\n",
    "3. 주어진 P, Q를 이용해 예측 평점 $\\hat{R}$를 구한다. ($\\matrix{P}$ x $\\mathbf{Q}^\\top$ = $\\hat{R}$)\n",
    "4. $\\matrix{R}$과 $\\hat{R}$을 비교해 Error를 구하고, 이를 이용해 P와 Q를 update한다(like using SGD)\n",
    "5. 전체오차가 미리 정해진 기준 값 이하가 되거나 지정된 iteration 횟수에 도달할 때 까지 3~4를 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, RMSE: 0.9585227481580039\n",
      "Iteration: 20, RMSE: 0.9373806501000475\n",
      "Iteration: 30, RMSE: 0.9281088106057349\n",
      "Iteration: 40, RMSE: 0.9225966253787458\n",
      "Iteration: 50, RMSE: 0.9185301934254234\n",
      "Iteration: 60, RMSE: 0.914779377630549\n",
      "Iteration: 70, RMSE: 0.9104629019332259\n",
      "Iteration: 80, RMSE: 0.9046382388563874\n",
      "Iteration: 90, RMSE: 0.8962972621456214\n",
      "Iteration: 100, RMSE: 0.8848484352881525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('/Users/jisujung/Desktop/dev/RecSys/python을 이용한 개인화 추천시스템/data/u.data', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "ratings= ratings[['user_id', 'movie_id', 'rating']].astype(int)\n",
    "\n",
    "class MF():\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, verbose=True):\n",
    "        self.R= np.array(ratings)\n",
    "        self.num_users, self.num_items= np.shape(self.R)\n",
    "        self.K= K\n",
    "        self.alpha= alpha\n",
    "        self.beta= beta\n",
    "        self.iterations= iterations\n",
    "        self.verbose= verbose\n",
    "\n",
    "    # RMSE 계산\n",
    "    def rmse(self):\n",
    "        x_idx, y_idx= self.R.nonzero()\n",
    "        self.predictions= []\n",
    "        self.errors= []\n",
    "        for x, y in zip(x_idx, y_idx):\n",
    "            prediction= self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions= np.array(self.predictions)\n",
    "        self.errors= np.array(self.errors)\n",
    "\n",
    "        return np.sqrt(np.mean(self.errors ** 2))\n",
    "\n",
    "    def train(self):\n",
    "        # Initializing user-feature and movie feature matrix (P, Q)\n",
    "        # 잘 보면 Q는 (item x feature)로 정의되어있는데, 책에서는 matrix Q는 코드랑 같게 되어있는데 element q가 Q의 element가 아니라 Q.T의 element인 것 처럼 정해지gim\n",
    "        self.P= np.random.normal(scale= 1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q= np.random.normal(scale= 1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # Initializing the bias terms\n",
    "        self.b_u= np.zeros(self.num_users)\n",
    "        self.b_d= np.zeros(self.num_items)\n",
    "        self.b= np.mean(self.R[self.R.nonzero()])\n",
    "\n",
    "        # List of training samples - train only nonzero values\n",
    "        rows, columns= self.R.nonzero()\n",
    "        self.samples= [(row, column, self.R[row, column]) for (row, column) in zip(rows, columns)]\n",
    "\n",
    "        # Stochastic GD for given number of iterations\n",
    "        training_process= []\n",
    "        for i in range(self.iterations):\n",
    "            # SGD는 시작점이 중요하므로 shuffle을 통해 순서를 바꿔준다.\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse= self.rmse()\n",
    "            training_process.append((i+1, rmse))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: {}, RMSE: {}\".format(i+1, rmse))\n",
    "\n",
    "    def get_prediction(self, x, y):\n",
    "        # R에는 zero(null) value가 있을지라도, b, b_u, b_d, P, Q는 그렇지 않다.\n",
    "        prediction= self.b + self.b_u[x] + self.b_d[y] + self.P[x, :].dot(self.Q[y, :].T)\n",
    "        return prediction\n",
    "\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction= self.get_prediction(i, j)\n",
    "            e= (r - prediction)     \n",
    "            self.b_u[i]+= self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j]+= self.alpha * (e - self.beta * self.b_d[j])\n",
    "\n",
    "            # error앞에 2 어디?\n",
    "            # 앞서 얘기했듯이, 책의 q가 잘못 서술되어있음. 책의 q는 Q의 element가 아니라 Q.T의 element임. w\n",
    "            self.P[i, :]+= self.alpha * (e*self.Q[j, :] - self.beta * self.P[i, :])\n",
    "            self.Q[j, :]+= self.alpha * (e*self.P[i, :] - self.beta * self.Q[j, :])\n",
    "\n",
    "\n",
    "# execute\n",
    "R_temp= ratings.pivot(index= 'user_id', columns= 'movie_id', values= 'rating').fillna(0)\n",
    "mf= MF(R_temp, 30, 0.001, 0.02, 100)\n",
    "train_process= mf.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training_Test Simultaneously\n",
    "\n",
    "Train/Test set을 분리하고,training을 진행하면서 중간중간 test set으로 성과를 측정한다.\n",
    "\n",
    "대부분의 경우, user/item의 id와 DataFrame(or Array)의 index가 일치하지 않는 경우가 많다. 이 경우, 나중에 id로 index를 조회할 때(혹은 그 역의 경우) 제대로 작동 할 수 없으므로, 미리 **실제 아이디와 내부 인덱스를 매핑**해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10, Training RMSE: 0.9659, Test RMSE: 0.9833\n",
      "Iterations: 20, Training RMSE: 0.9410, Test RMSE: 0.9644\n",
      "Iterations: 30, Training RMSE: 0.9298, Test RMSE: 0.9566\n",
      "Iterations: 40, Training RMSE: 0.9231, Test RMSE: 0.9523\n",
      "Iterations: 50, Training RMSE: 0.9183, Test RMSE: 0.9496\n",
      "Iterations: 60, Training RMSE: 0.9145, Test RMSE: 0.9478\n",
      "Iterations: 70, Training RMSE: 0.9109, Test RMSE: 0.9465\n",
      "Iterations: 80, Training RMSE: 0.9071, Test RMSE: 0.9453\n",
      "Iterations: 90, Training RMSE: 0.9025, Test RMSE: 0.9440\n",
      "Iterations: 100, Training RMSE: 0.8967, Test RMSE: 0.9425\n",
      "\n",
      "3.3794437988052235\n"
     ]
    }
   ],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('/Users/jisujung/Desktop/dev/RecSys/python을 이용한 개인화 추천시스템/data/u.data', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "ratings= ratings[['user_id', 'movie_id', 'rating']].astype(int)\n",
    "\n",
    "# Train-Test-Split\n",
    "from sklearn.utils import shuffle\n",
    "TRAIN_SIZE= 0.75\n",
    "ratings= shuffle(ratings, random_state= 1)\n",
    "cutoff= int(TRAIN_SIZE * len(ratings))\n",
    "# pivot하기 전에 shuffle! 생각해봐... pivot한 형태면 어떻게 쪼개....\n",
    "ratings_train= ratings.iloc[:cutoff]\n",
    "ratings_test= ratings.iloc[cutoff:]\n",
    "\n",
    "# New MF class for training & testing\n",
    "# user_id, item_id와 dataframe(or array)의 index를 mapping해야 함!\n",
    "class New_MF():\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, verbose=True):\n",
    "        self.R= np.array(ratings)\n",
    "\n",
    "        item_id_index= []\n",
    "        index_item_id= []\n",
    "        for idx, one_id in enumerate(ratings):\n",
    "            item_id_index.append([one_id, idx])\n",
    "            index_item_id.append([idx, one_id])\n",
    "        self.item_id_index= dict(item_id_index)\n",
    "        self.index_item_id= dict(index_item_id)\n",
    "\n",
    "        user_id_index= []\n",
    "        index_user_id= []\n",
    "        for idx, one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id, idx])\n",
    "            index_user_id.append([idx, one_id])\n",
    "        self.user_id_index= dict(user_id_index)\n",
    "        self.index_user_id= dict(index_user_id)\n",
    "\n",
    "        self.num_users, self.num_items= np.shape(self.R)\n",
    "        self.K= K\n",
    "        self.alpha= alpha\n",
    "        self.beta= beta\n",
    "        self.iterations= iterations\n",
    "        self.verbose= verbose\n",
    "\n",
    "    def rmse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x, y in zip(xs, ys):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "\n",
    "    # Ratings for user i and item j\n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    # SGD to get Optimized b_u, b_d, P, Q\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            e = (r - prediction)\n",
    "\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "\n",
    "    # Test Set\n",
    "    def set_test(self, ratings_test):\n",
    "        test_set= []\n",
    "        for i in range(len(ratings_test)):\n",
    "            # x, y는 각각 user와 item의 index\n",
    "            # (i, j, value)의 tuple을 list에 담음으로서 이전 코드와의 통일성을 유지 할 수 있음.\n",
    "            x= self.user_id_index[ratings_test.iloc[i, 0]]\n",
    "            y= self.item_id_index[ratings_test.iloc[i, 1]]\n",
    "            z= ratings_test.iloc[i, 2]\n",
    "            test_set.append([x, y, z])\n",
    "            # 0으로 바꿔주면 train시 이 부분 건너 뜀\n",
    "            self.R[x, y]= 0\n",
    "        self.test_set= test_set\n",
    "\n",
    "        return test_set\n",
    "\n",
    "    def test_rmse(self):\n",
    "        error= 0\n",
    "        for one_set in self.test_set:\n",
    "            predicted= self.get_prediction(one_set[0], one_set[1])\n",
    "            error += pow(one_set[2] - predicted, 2)\n",
    "\n",
    "        return np.sqrt(error/len(self.test_set))\n",
    "\n",
    "    def test(self):\n",
    "        # Initializing P, Q, b_u, b_d\n",
    "        self.P= np.random.normal(scale= 1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q= np.random.normal(scale= 1./self.K, size=(self.num_items, self.K))\n",
    "        self.b_u= np.zeros(self.num_users)\n",
    "        self.b_d= np.zeros(self.num_items)\n",
    "        self.b= np.mean(self.R[self.R.nonzero()])\n",
    "        \n",
    "        row, columns= self.R.nonzero()\n",
    "        self.samples= [(i, j, self.R[i, j]) for (i, j) in zip(row, columns)]\n",
    "        \n",
    "        # run SGD\n",
    "        training_process= []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse1= self.rmse()\n",
    "            rmse2= self.test_rmse()\n",
    "            training_process.append((i+1, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iterations: %d, Training RMSE: %.4f, Test RMSE: %.4f\" % (i+1, rmse1, rmse2))\n",
    "        \n",
    "        return training_process\n",
    "    \n",
    "    def get_one_prediction(self, user_id, item_id):\n",
    "        return self.get_prediction(self.user_id_index[user_id], self.item_id_index[item_id])\n",
    "        \n",
    "    def full_prediction(self):    \n",
    "        return self.b + self.b_u[:, np.newaxis], self.b_d[np.newaxis, :] + self.P.dot(self.Q.T)\n",
    "    \n",
    "\n",
    "R_temp= ratings.pivot(index= 'user_id', columns= 'movie_id', values= 'rating').fillna(0)\n",
    "mf= New_MF(R_temp, K=30, alpha=0.001, beta=0.02, iterations=100, verbose=True)\n",
    "test_set=  mf.set_test(ratings_test)\n",
    "result= mf.test()\n",
    "\n",
    "print('')\n",
    "print(mf.get_one_prediction(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parameter Tuning\n",
    "(실제로 코드를 돌려보기에는 조금 무리가 있는 연산량이므로 코드만 짜고 출력은 하지 않음)\n",
    "\n",
    "이번 예시에서는 최적의 K와 iteration 횟수를 찾아본다.\n",
    "\n",
    "### K\n",
    "K는 latent factor의 수를 의미하므로, K가 크면 클수록 다양한 패턴으로 평가 데이터를 설명 할 수 있지만, Training set에 대해 overfitting될 가능성이 커진다.\n",
    "\n",
    "따라서, Test set의 정확도는 k가 증가함에 따라 같이 증가하였다가 어느 순간 감소할 것이다.\n",
    "\n",
    "#### How to Find Optimal K\n",
    "최적의 K를 찾기 위해, 50~260까지의 넓은 범위에 대해 10 간격으로 정확도(RMSE)를 계산한다(그 후 다시 1간격으로 더 정확한 optimal k를 찾을 수 있겠지만, 여기서는 생략한다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "index= []\n",
    "R_temp= ratings.pivot(index= 'user_id', columns= 'movie_id', values= 'rating').fillna(0)\n",
    "for k in range(50, 261, 10):\n",
    "    mf= New_MF(R_temp, K= k, alpha= 0.001, beta= 0.02, iterations= 300)\n",
    "    test_set= mf.set_test(ratings_test)\n",
    "    result= mf.test()[2]\n",
    "    index.append(k)\n",
    "    results.append(result)\n",
    "    \n",
    "opt_idx= np.argmax(results)\n",
    "opt_k= index[opt_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations\n",
    "Iterations는 trainig 횟수를 의미하므로, Iterations가 크면 클수록 학습이 더 정확하게 이루어지지만, Training set에 대해 overfitting될 가능성이 커진다. 따라서 Test set의 정확도는 iterations가 증가함에 따라 같이 증가하였다가 어느 순간 감소할 것이다.\n",
    "\n",
    "#### How to Find Optimal Iterations\n",
    "학습과정에서 iterations에 충분히 큰 숫자를 주고, RMSE의 변화 양상을 관찰하여 주어진 K에 대한 최적의 iteration 값을 구한다. 이 때, K는 위에서 구한 optimal value를 사용한다.\n",
    "\n",
    "가장 정확하게 optimal한 (k, iterations)의 조합을 구하기 위해서는 위에서 찾은 K로 고정하는 것이 아니라 다양한 조합을 시도해보아야 하지만, 이 경우 연산량이 폭발적으로 증가하므로 보통 이와 같이 순차적으로 최적의 parameter를 구해나간다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
