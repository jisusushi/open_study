{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation for **``Training Deep AutoEncoders for Collaborative Filtering Implementation``**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Model\n",
    "* **Loss**: Define loss function for model\n",
    "* **TrainTestDataset**: Define ``Dataset`` class by overwriting ``__len__()`` and ``__getitem__()``\n",
    "* **Model**: Construct a AutoEncoder Model\n",
    "* **Main**: Train & Test the model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss.py\n",
    "\n",
    "보편적인 loss함수와는 다르게, 본 model에서는 actual rating이 없는 경우에 대해서는 loss를 평가 할 수가 없다(약간 unsupervised learning 느낌? 사용자가 현실에서 평가를 하기 전까지는 정답을 알 수 없으니까).\n",
    "\n",
    "따라서 ``torch.nn``에 있는 loss function이 아닌, 본 상황에 맞는 loss function을 새롭게 정의하여야 한다. \n",
    "\n",
    "특히, 본 Paper에서는 **Masked Mean Squared Error**를 사용한다.\n",
    "\n",
    "$$ MMSE = {m_i * (r_i - y_i)^2 \\over \\sum_{i=0}^{i=n} m_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEloss_with_Mask(nn.Module):\n",
    "        def __init__(self):\n",
    "        super(MSEloss_with_Mask, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Masking into a vector of 1's and 0's.\n",
    "        mask= (targets!=0)\n",
    "        mask= mask.float()\n",
    "\n",
    "        # actual number of ratings\n",
    "        # Take max to avoid division by zero while caculating loss\n",
    "        other= torch.Tensor([1.0])\n",
    "        number_ratings= torch.max(torch.sum(mask), other)\n",
    "        error= torch.sum(torch.mul(mask, torch.mul((targets-inputs), (targets-inputs))))\n",
    "        loss= error.div(number_ratings)\n",
    "        return loss[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example\n",
    "#### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True, False,  True, False,  True])\n"
     ]
    }
   ],
   "source": [
    "targets_= torch.Tensor([5, 3, 0, 4, 0, 5])\n",
    "mask_= (targets!=0)\n",
    "print(mask_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## TrainTestDataset.py\n",
    "\n",
    "Transformation을 적용 할 수 있는 ``Dataset`` class를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDataset(Dataset):\n",
    "        def __init__(self, file, transform=None):\n",
    "        self.data= pd.read_csv(file)\n",
    "        self.data= self.data.iloc[:, 1:]\n",
    "        self.transform= transform\n",
    "\n",
    "        if transform is not None:\n",
    "            self.data= self.transform(np.array(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        user_vector= self.data.data[0][ind]\n",
    "\n",
    "        return user_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Model.py\n",
    "\n",
    "#### AutoEncoder Class\n",
    "**``AutoEncoder``**를 정의한다. AutoEncoder의 input으로는 다음과 같은 parameter가 들어간다.\n",
    "* layer_size: size of each layer in the AE model\n",
    "        ex) [10000, 1024, 512] result in\n",
    "            - encoder 2 layers: 10000 x 1024, 1024 x 512\n",
    "            - representation layer z: 512\n",
    "            - decoder 2 layers: 512 x 1024, 1024 x 10000\n",
    "* nl_type: non-linearity activation function type\n",
    "* is_constrained: if True, the the weights of encoder and decoder are tied\n",
    "* dp_drop_prob: dropout probability. if > 0, dropout process proceeds\n",
    "* last_layer_activation: whether to apply activation on last decode layer\n",
    "\n",
    "\n",
    "#### Weights and Biases\n",
    "**``nn.ParameterList``** class와 ``layer_size`` parameter를 통해 **encoder/decoder의 weight/bias**를 직접 초기화해준다.\n",
    "\n",
    "\n",
    "#### Stacked AutoEncoder\n",
    "**``is_constrained``**의 값에 따라 decoder를 정의하고 학습하는 방법이 달라진다.\n",
    "\n",
    "\n",
    "#### Activation Funciton\n",
    "paper에서는 ``selu``를 사용하였지만, 다양한 activation funciton의 사용을 위해 함수를 하나 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(input, type):\n",
    "    if type.lower() == 'selu':\n",
    "        return F.selu(input)\n",
    "    elif type.lower() == 'elu':\n",
    "        return F.elu(input)\n",
    "    elif type.lower() == 'relu':\n",
    "        return F.relu(input)\n",
    "    elif type.lower() == 'relu6':\n",
    "        return F.relu6(input)\n",
    "    elif type.lower() == 'tanh':\n",
    "        return F.tanh(input)\n",
    "    elif type.lower() == 'sigmoid':\n",
    "        return F.sigmoid(input)\n",
    "    elif type.lower() == 'swish':\n",
    "        return F.sigmoid(input) * input\n",
    "    elif type.lower() == 'identity':\n",
    "        return input\n",
    "    else:\n",
    "        raise ValueError(\"Unknown non-Linearity activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, layer_size, nl_type='selu', is_constrained=True, dp_drop_prob= 0, last_layer_activations=True):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        '''\n",
    "        layer_sizes: size of each layer in the autoencoder model\n",
    "            ex) [10000, 1024, 512] will result in\n",
    "                - encoder 2 layers: 10000 x 1024 & 1024 x 512\n",
    "                - representation layer z: 512\n",
    "                - decoder 2 layers: 512 x 1024 & 1024 x 10000\n",
    "        nl_type: non-linearity type\n",
    "        is_constrained: if ture then the weights of encoder and decoder are tied\n",
    "        dp_drop_prob: dropout probability\n",
    "        last_layer_activations: whether to apply activation on last decoder layer\n",
    "        '''\n",
    "\n",
    "        self.layer_sizes= layer_size\n",
    "        self.nl_type= nl_type\n",
    "        self.is_constrained= is_constrained\n",
    "        self.dp_drop_prob= dp_drop_prob\n",
    "        self.last_layer_activations= last_layer_activations\n",
    "\n",
    "        if dp_drop_prob > 0:\n",
    "            self.drop= nn.Dropout(dp_drop_prob)\n",
    "\n",
    "        self._last= len(layer_size) - 2\n",
    "\n",
    "        # initialize weights\n",
    "        self.encoder_weights= nn.ParameterList([nn.Parameter(torch.rand(layer_size[i+1], layer_size[i])) for i in range(len(layer_size)-1)])\n",
    "\n",
    "        for weights in self.encoder_weights:\n",
    "            init.xavier_uniform_(weights)\n",
    "\n",
    "        self.encoder_bias= nn.ParameterList([nn.Parameter(torch.zeros(layer_size[i+1])) for i in range(len(layer_size) - 1)])\n",
    "\n",
    "        reverse_layer_sizes= list(reversed(layer_size))\n",
    "\n",
    "        # Decoder weights\n",
    "        if is_constrained == False:\n",
    "            self.decoder_weights= nn.ParameterList([nn.Parameter(torch.rand(reverse_layer_sizes[i+1], reverse_layer_sizes[i])) for i in range(len(reverse_layer_sizes) - 1)])\n",
    "\n",
    "            for weights in self.decoder_weights:\n",
    "                init.xavier_uniform_(weights)\n",
    "\n",
    "        self.decoder_bias= nn.ParameterList([nn.Parameter(torch.zeros(reverse_layer_sizes[i+1])) for i in range(len(reverse_layer_sizes) - 1)])\n",
    "\n",
    "    def encode(self, x):\n",
    "        for i, w in enumerate(self.encoder_weights):\n",
    "            x= F.linear(input=x, weight=w, bias=self.encoder_bias[i])\n",
    "            x= activation(input=x, type=self.nl_type)\n",
    "\n",
    "        if self.dp_drop_prob > 0:\n",
    "            x= self.drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        if self.is_constrained == True:\n",
    "            for i, w in zip(range(len(self.encoder_weights)), list(reversed(self.encoder_weights))):\n",
    "                x= F.linear(input=x, weight=w.t(), bias=self.decoder_bias[i])\n",
    "                x= activation(input=x, type=self.nl_type if i != self._last or self.last_layer_activations else 'identity')\n",
    "\n",
    "        else:\n",
    "            for i, w in enumerate(self.decoder_weights):\n",
    "                x= F.linear(input=x, weight=w, bias=self.decoder_bias[i])\n",
    "                x= activation(input=x, type=self.nl_type if i != self._last or self.last_layer_activations else 'identity')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 4. Main.py\n",
    "Model을 위해 data를 준비하고 학습하기 위해 다음과 같은 과정이 필요하다.\n",
    "1. Data 준비 - DataLoader\n",
    "2. Model 설계 - layer 수, loss function, optimizer\n",
    "3. Training\n",
    "4. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "앞서 정의한 TrainTestDataset을 이용하자! data에 가해줄 transformation을 정의하고, DataLoader를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations= transforms.Compose([transforms.ToTensor()])\n",
    "train_dat= TrainTestDataset('./data/train.csv', transformations)\n",
    "test_dat= TrainTestDataset('./data/test.csv', transformations)\n",
    "\n",
    "train_dl= DataLoader(dataset=train_dat, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_dl= DataLoader(dataset=test_dat, batch_size=512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "layer의 size를 정하고 model을 불러오며, loss function과 optimizer등 training에 필요한 요소들을 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes= [9559, 512, 512, 1024]\n",
    "model=AutoEncoder(layer_size=layer_sizes, nl_type='selu', is_constrained=True, dp_drop_prob=0.0, last_layer_activations=False)\n",
    "\n",
    "criterion= MSEloss_with_Mask()\n",
    "optimizer= optim.Adam(model.parameters(), lr= 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validaiton\n",
    "DataLoader도 준비되었고, model도 정의되었으니 이제 학습과 validation을 진행 할 수 있다.\n",
    "\n",
    "매 epoch마다 training과 validation을 반복한다. 이 과정은 딱히 특별할게 없다. batch input을 model에 통과시키고, loss를 계산하고, backward pass를 수행한 이후 optimizer의 step function을 이용해 weight을 update한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dl, test_dl, num_epochs= 40):\n",
    "    liveloss= PlotLosses()\n",
    "    lr2_tr_loss, lr2_val_loss= [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, valid_loss= [], []\n",
    "        logs= {}\n",
    "        prefix= ''\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "            inputs = labels= data\n",
    "            inputs= inputs.float()\n",
    "            labels= labels.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs= model(inputs)\n",
    "            loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Iterative Dense Output Re-Feeding\n",
    "            for iter_ in range(3):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs= model(outputs.detach())\n",
    "                loss= criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            logs[prefix+\"MME loss\"]= loss.item()\n",
    "\n",
    "        for i, data in enumerate(test_dl):\n",
    "            model.eval()\n",
    "            inputs = labels= data\n",
    "            inputs= inputs.float()\n",
    "            labels= labels.float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs= model(inputs)\n",
    "                loss= criterion(outputs, labels)\n",
    "                valid_loss.append(loss.item())\n",
    "                prefix= 'val_'\n",
    "                logs[prefix + \"MMSE loss\"]= loss.item()\n",
    "\n",
    "        lr2_tr_loss.append(np.mean(train_loss))\n",
    "        lr2_val_loss.append(np.mean(valid_loss))\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "        print(\"Epoch:\", epoch+1, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))\n",
    "\n",
    "        if epoch == num_epochs -1:\n",
    "            return lr2_tr_loss, lr2_val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
